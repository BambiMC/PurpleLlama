"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[382],{2366:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"tutorials/standalone-agent-llamafirewall-tutorial","title":"Notebook: Standalone Agent with LlamaFirewall","description":"This demo showcases a standalone agent implementation that integrates LlamaFirewall for security scanning. The agent can interact with users through a console interface, perform web searches, and fetch URL content while ensuring security through LlamaFirewall\'s scanning capabilities.","source":"@site/docs/tutorials/standalone-agent-llamafirewall-tutorial.md","sourceDirName":"tutorials","slug":"/tutorials/standalone-agent-llamafirewall-tutorial","permalink":"/PurpleLlama/LlamaFirewall/docs/tutorials/standalone-agent-llamafirewall-tutorial","draft":false,"unlisted":false,"editUrl":"https://github.com/meta-llama/PurpleLlama/tree/main/LlamaFirewall/website/docs/tutorials/standalone-agent-llamafirewall-tutorial.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialsSidebar","previous":{"title":"Prompt Regex Scanner Tutorial","permalink":"/PurpleLlama/LlamaFirewall/docs/tutorials/regex-scanner-tutorial"}}');var s=t(4848),r=t(8453);const o={jupyterCodeCell:"jupyterCodeCell_HwFd",regularCode:"regularCode_ONcK"},l={sidebar_position:5},i="Notebook: Standalone Agent with LlamaFirewall",c={},m=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Setup",id:"setup",level:2},{value:"1. Create a Conda Environment",id:"1-create-a-conda-environment",level:3},{value:"2. Install Required Packages",id:"2-install-required-packages",level:3},{value:"3. Configure Environment Variables",id:"3-configure-environment-variables",level:3},{value:"Features",id:"features",level:2},{value:"LlamaFirewall Integration",id:"llamafirewall-integration",level:3},{value:"Available Tools",id:"available-tools",level:3},{value:"How It Works",id:"how-it-works",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"notebook-standalone-agent-with-llamafirewall",children:"Notebook: Standalone Agent with LlamaFirewall"})}),"\n",(0,s.jsx)(n.p,{children:"This demo showcases a standalone agent implementation that integrates LlamaFirewall for security scanning. The agent can interact with users through a console interface, perform web searches, and fetch URL content while ensuring security through LlamaFirewall's scanning capabilities."}),"\n",(0,s.jsxs)(n.p,{children:["NOTE: You can download this as a Jupyter Notebook from the ",(0,s.jsx)(n.a,{href:"https://github.com/meta-llama/PurpleLlama/tree/main/LlamaFirewall/notebook",children:"notebook"})," directory in the repo."]}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The Demo Standalone Agent is a Python application that:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Connects to the ",(0,s.jsx)(n.a,{href:"https://llama.developer.meta.com/join_waitlist?utm_source=purple_llama&utm_medium=notebook&utm_campaign=agent_demo",children:"Llama API"})," for LLM capabilities"]}),"\n",(0,s.jsx)(n.li,{children:"Integrates LlamaFirewall for security scanning of messages"}),"\n",(0,s.jsx)(n.li,{children:"Provides web search functionality using Tavily API"}),"\n",(0,s.jsx)(n.li,{children:"Can fetch and process URL content"}),"\n",(0,s.jsx)(n.li,{children:"Runs in an interactive console mode"}),"\n",(0,s.jsx)(n.li,{children:"Demonstrates tool usage with LLM agents"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python 3.10 or higher"}),"\n",(0,s.jsx)(n.li,{children:"Conda (recommended for environment management)"}),"\n",(0,s.jsxs)(n.li,{children:["Access to ",(0,s.jsx)(n.a,{href:"https://llama.developer.meta.com/join_waitlist?utm_source=purple_llama&utm_medium=notebook&utm_campaign=agent_demo",children:"Llama API"})]}),"\n",(0,s.jsx)(n.li,{children:"Tavily API key (optional, for web search functionality)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsx)(n.h3,{id:"1-create-a-conda-environment",children:"1. Create a Conda Environment"}),"\n",(0,s.jsx)("div",{className:o.regularCode,children:(0,s.jsx)(n.p,{children:"conda create -n llamafirewall python=3.10 -y\nconda activate llamafirewall"})}),"\n",(0,s.jsx)(n.h3,{id:"2-install-required-packages",children:"2. Install Required Packages"}),"\n",(0,s.jsx)("div",{className:o.regularCode,children:(0,s.jsx)(n.p,{children:"pip install llamafirewall openai python-dotenv rich tavily-python"})}),"\n",(0,s.jsx)(n.h3,{id:"3-configure-environment-variables",children:"3. Configure Environment Variables"}),"\n",(0,s.jsxs)(n.p,{children:["Create a ",(0,s.jsx)(n.code,{children:".env"})," file in the same directory as the script with the following variables:"]}),"\n",(0,s.jsxs)("div",{className:o.regularCode,children:[(0,s.jsx)(n.h1,{id:"llama-api-configuration",children:"Llama API Configuration"}),(0,s.jsxs)(n.p,{children:["LLAMA_API_KEY=your_llama_api_key\nLLAMA_API_BASE_URL=",(0,s.jsx)(n.a,{href:"https://api.llama.com/compat/v1/",children:"https://api.llama.com/compat/v1/"}),"\nLLAMA_API_MODEL=Llama-4-Maverick-17B-128E-Instruct-FP8"]}),(0,s.jsx)(n.h1,{id:"tavily-api-configuration-optional-for-web-search",children:"Tavily API Configuration (optional, for web search)"}),(0,s.jsx)(n.p,{children:"TAVILY_API_KEY=your_tavily_api_key"})]}),"\n",(0,s.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,s.jsx)(n.h3,{id:"llamafirewall-integration",children:"LlamaFirewall Integration"}),"\n",(0,s.jsx)(n.p,{children:"The demo uses LlamaFirewall to scan messages for security issues:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Code Shield scanner for code-related security issues"}),"\n",(0,s.jsx)(n.li,{children:"Llama Prompt Guard for user and tool input to the loop"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"available-tools",children:"Available Tools"}),"\n",(0,s.jsx)(n.p,{children:"The agent has access to the following tools:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"websearch"}),": Performs web searches using the Tavily API"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"fetch_url_contents"}),": Fetches HTML content from a specified URL"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"The agent initializes LlamaFirewall with appropriate scanners for different message roles"}),"\n",(0,s.jsx)(n.li,{children:"When a user sends a message, it's scanned by LlamaFirewall"}),"\n",(0,s.jsx)(n.li,{children:"If the message passes the scan, it's sent to the Llama API"}),"\n",(0,s.jsx)(n.li,{children:"The API may respond with tool calls, which are executed by the agent"}),"\n",(0,s.jsx)(n.li,{children:"Tool results are also scanned by LlamaFirewall before being sent back to the API"}),"\n",(0,s.jsx)(n.li,{children:"The process continues until the API provides a final response without tool calls"}),"\n",(0,s.jsx)(n.li,{children:"The response is displayed to the user"}),"\n"]}),"\n",(0,s.jsx)("div",{className:o.jupyterCodeCell,children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{showLineNumbers}",children:"# use if the notebook is executing in a conda env without the packages installed\n# !pip install llamafirewall openai python-dotenv rich tavily-python\n"})})}),"\n",(0,s.jsx)("div",{className:o.jupyterCodeCell,children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{showLineNumbers}",children:'import os\nfrom dotenv import load_dotenv\n\ntry:\n  from google.colab import userdata\n  in_colab = True\nexcept ImportError:\n  in_colab = False\n\nif in_colab:\n  # setup your keys in the colab secrets UI\n  secrets_to_load = [ "TAVILY_SEARCH_API_KEY", "LLAMA_API_KEY", "LLAMA_API_BASE_URL", "LLAMA_API_MODEL"]\n\n  for secret_name in secrets_to_load:\n    secret_value = userdata.get(secret_name)\n    if secret_value:\n      os.environ[secret_name] = secret_value\n    else:\n      print(f"Warning: Secret \'{secret_name}\' not found in userdata.")\nelse:\n    # Load environment variables from .env file\n    load_dotenv()\n'})})}),"\n",(0,s.jsx)("div",{className:o.jupyterCodeCell,children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{showLineNumbers}",children:'# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nimport traceback\nimport requests\n\nfrom typing import Any, Callable, Dict, List\nfrom rich import print\n\nfrom openai import OpenAI\n\nfrom llamafirewall import LlamaFirewall, ScanDecision, ScannerType\nfrom llamafirewall.llamafirewall_data_types import (\n    AssistantMessage,\n    Role,\n    ToolMessage,\n    UserMessage,\n)\n\n# logging configs\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARN)  # Set the logging level\nlogger.propagate = False\nformatter = logging.Formatter(\n    fmt="%(asctime)s %(levelname)s: %(message)s", datefmt="%Y-%m-%d - %H:%M:%S"\n)\nconsole_handler = logging.StreamHandler(sys.stdout)\nconsole_handler.setLevel(logging.DEBUG)\nconsole_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\n\n\n# Note: To use the websearch and extract_url functions, install the tavily package:\n# pip install tavily-python\ntry:\n    from tavily import TavilyClient\n\n    TAVILY_AVAILABLE = True\nexcept ImportError:\n    TAVILY_AVAILABLE = False\n    logger.warning(\n        "[bold yellow]Warning: Tavily package not found. Web search functionality will be limited.[/bold yellow]"\n    )\n\n\n\n\n@nocommit - await for Beto\'s updates\n# Initialize the client with the API key and base URL from the .env file, the are from the Llama API NOT OPENAI\nclient = OpenAI(\n    api_key=os.getenv("LLAMA_API_KEY"), base_url=os.getenv("LLAMA_API_BASE_URL")\n)\n\n# Initialize LlamaFirewall\nllama_firewall = LlamaFirewall()\n\n\ndef function_to_tool_schema(func: Callable) -> Dict[str, Any]:\n    """\n    Convert a Python function to the tool schema format required by LlamaAPIClient.\n\n    Args:\n        func: The Python function to convert\n\n    Returns:\n        A dictionary representing the tool schema\n    """\n    # Get function signature\n    sig = inspect.signature(func)\n\n    # Get function docstring for description\n    description = inspect.getdoc(func) or f"Function to {func.__name__}"\n\n    # Build parameters schema\n    properties = {}\n    required = []\n\n    for param_name, param in sig.parameters.items():\n        # Skip self parameter for methods\n        if param_name == "self":\n            continue\n\n        param_type = "string"  # Default type\n        param_desc = ""\n\n        # Try to get type annotation\n        if param.annotation != inspect.Parameter.empty:\n            if param.annotation == str:\n                param_type = "string"\n            elif param.annotation == int:\n                param_type = "integer"\n            elif param.annotation == float:\n                param_type = "number"\n            elif param.annotation == bool:\n                param_type = "boolean"\n\n        # Add parameter to properties\n        properties[param_name] = {\n            "type": param_type,\n            "description": param_desc\n            or f"Parameter {param_name} for function {func.__name__}",\n        }\n\n        # Add to required list if no default value\n        if param.default == inspect.Parameter.empty:\n            required.append(param_name)\n\n    # Build the complete tool schema\n    tool_schema = {\n        "type": "function",\n        "function": {\n            "name": func.__name__,\n            "description": description,\n            "parameters": {\n                "type": "object",\n                "properties": properties,\n                "required": required,\n                "additionalProperties": False,\n            },\n            "strict": True,\n        },\n    }\n\n    return tool_schema\n\n\ndef websearch(query: str) -> str:\n    """\n    Perform a web search for the given query using Tavily API.\n\n    Args:\n        query: The search query\n\n    Returns:\n        Search results from Tavily\n    """\n    if not TAVILY_AVAILABLE:\n        return "Error: Tavily package not installed. Please install with \'pip install tavily-python\'."\n\n    tavily_api_key = os.getenv("TAVILY_API_KEY")\n    if not tavily_api_key:\n        return "Error: Tavily API key not found in environment variables. Please add TAVILY_API_KEY to your .env file."\n\n    try:\n        # Initialize Tavily client\n        tavily_client = TavilyClient(api_key=tavily_api_key)\n\n        # Perform search\n        result = tavily_client.search(query, search_depth="basic", max_results=5)\n\n        # Format the results\n        formatted_results = f"Search results for \'{query}\':\\n\\n"\n\n        for i, item in enumerate(result.get("results", []), 1):\n            title = item.get("title", "No title")\n            content = item.get("content", "No content")\n            url = item.get("url", "No URL")\n\n            formatted_results += f"{i}. **{title}**\\n"\n            formatted_results += f"   {content[:200]}...\\n"\n            formatted_results += f"   Source: {url}\\n\\n"\n\n        if not result.get("results"):\n            formatted_results += "No results found."\n\n        return formatted_results\n\n    except Exception as e:\n        return f"Error performing web search: {str(e)}"\n\n\ndef fetch_url_content_pre_rendering(url):\n    """\n    Fetches the HTML content from the provided URL with additional logging for debugging.\n\n    Args:\n        url (str): The URL of the page to fetch.\n\n    Returns:\n        str: The HTML content of the page if successful.\n\n    Raises:\n        requests.RequestException: If the HTTP request fails.\n    """\n\n    try:\n        logger.debug("Attempting to fetch URL: %s", url)\n\n        response = requests.get(url)\n\n        logger.debug("Received response with status code: %d", response.status_code)\n        logger.debug("Sample content %s", response.text[:1000])\n\n        # This will raise an HTTPError for 400, 500, etc.\n        response.raise_for_status()\n\n        html = response.text\n        logger.debug(\n            "Successfully fetched content; content length: %d characters", len(html)\n        )\n        return html\n\n    except requests.RequestException as e:\n        logger.error("Request failed with error: %s", e)\n\n        # If we have a response object, log part of its text for debugging.\n        if "response" in locals() and response is not None:\n            content_preview = response.text[:500]  # print first 500 characters\n            logger.error("Response content preview: %s", content_preview)\n        raise\n\n\nasync def interact_with_ai(\n    user_message: str,\n    messages: List[Dict[str, Any]],\n    model="Llama-4-Scout-17B-16E-Instruct-FP8",\n) -> List[Dict[str, Any]]:\n    """\n    Interact with AI agent using OpenAI API with tools.\n\n    Args:\n        user_message: User\'s message\n        history: Chat history as a list of Message objects\n\n    Returns:\n        Updated chat history as a list of Message objects\n    """\n    # Define available tools\n    available_tools: Dict[str, Callable] = {\n        "websearch": websearch,\n        "fetch_url_content_pre_rendering": fetch_url_content_pre_rendering,\n    }\n\n    # Convert functions to tool schemas\n    tools = [function_to_tool_schema(func) for func in available_tools.values()]\n\n    # Add the current user message to our history\n    user_msg = UserMessage(content=user_message)\n\n    messages.append(\n        {\n            "role": Role.USER.value,\n            "content": user_message,\n        }\n    )\n\n    # Scan the user message\n    result = await llama_firewall.scan_async(user_msg)\n\n    if result.decision == ScanDecision.BLOCK:\n        response = f"Blocked by LlamaFirewall, {result.reason}"\n        messages.append(\n            {\n                "role": Role.ASSISTANT.value,\n                "content": response,\n            }\n        )\n\n        return messages\n\n    try:\n        # Start the Agent loop\n        tool_call = True\n        while tool_call:\n            # debug last message\n            logger.debug(f"messages: {messages[-1]}")\n\n            # Call Llama API with tools enabled\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                max_tokens=2048,\n                temperature=0.7,\n                tools=tools,\n            )\n\n            logger.debug(f"response: {response}")\n            # Get the response message\n            response_message = response.choices[0].message\n\n            messages.append(\n                {\n                    "role": response_message.role,\n                    "content": response_message.content,\n                    "tool_calls": response_message.tool_calls,\n                }\n            )\n\n            # Check if there are tool calls\n            if hasattr(response_message, "tool_calls") and response_message.tool_calls:\n                # Process each tool call\n                for tool_call in response_message.tool_calls:\n                    # Execute the tool\n                    tool_name = tool_call.function.name\n                    if tool_name in available_tools:\n                        try:\n                            # Parse arguments\n                            arguments = json.loads(tool_call.function.arguments)\n\n                            # Call the function\n                            tool_result = available_tools[tool_name](**arguments)\n\n                            tool_msg = ToolMessage(content=str(tool_result))\n\n                            # Scan the tool result\n                            lf_result = await llama_firewall.scan_async(tool_msg)\n\n                            if lf_result.decision == ScanDecision.BLOCK:\n                                # for demonstration purposes, we show the tool output, but this should be avoided in production\n                                blocked_response = f"Blocked by LlamaFirewall: {lf_result.reason} - Tool result: {tool_result}"\n                                tool_result = blocked_response\n\n                            # Add tool result to messages\n                            messages.append(\n                                {\n                                    "role": Role.TOOL.value,\n                                    "tool_call_id": tool_call.id,\n                                    "content": tool_result,\n                                }\n                            )\n\n                        except Exception as e:\n                            # Add error message if tool execution fails\n                            error_msg = f"Error: {str(e)}"\n                            tool_msg = AssistantMessage(content=error_msg)\n                            messages.append(\n                                {\n                                    "role": Role.TOOL.value,\n                                    "tool_call_id": tool_call.id,\n                                    "content": error_msg,\n                                }\n                            )\n                            logger.error(f"\\nTool Error: {error_msg}")\n\n                        logger.debug(f"Tool Call response: {messages[-1]}")\n\n                # Continue the loop to get the next response\n                continue\n            else:\n                # No tool calls, add the response to history and break the loop\n                tool_call = False\n                full_response = response_message.content\n                messages.append(\n                    {\n                        "role": Role.ASSISTANT.value,\n                        "content": full_response,\n                    }\n                )\n\n    except Exception as e:\n        error_message = f"Error calling API: {str(e)}"\n        traceback.print_exc()\n        messages.append(\n            {\n                "role": Role.ASSISTANT.value,\n                "content": error_message,\n            }\n        )\n\n    logger.debug(f"Assistant response: {messages[-1]}")\n\n    return messages\n\n\nasync def run_demo():\n    """\n    Run the agent in console mode, taking input from the command line.\n    """\n    print(\n        "[bold green]Starting AI Agent in console mode. Type \'exit\' to quit.[/bold green]"\n    )\n    messages: List[Dict[str, Any]] = []\n\n    model = os.getenv("LLAMA_API_MODEL")\n\n    while True:\n        user_message = input("\\nYou: ")\n        if user_message.lower() == "exit":\n            break\n\n        messages = await interact_with_ai(user_message, messages, model)\n\n        logger.info(f"Full history: {messages}")\n\n        # Print the assistant\'s response\n        if messages:\n            assistant_message = messages[-1]["content"]\n            print(f"\\n[bold green]Assistant:[/bold green] {assistant_message}")\n'})})}),"\n",(0,s.jsx)("div",{className:o.jupyterCodeCell,children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:"{showLineNumbers}",children:"await run_demo()\n"})})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var a=t(6540);const s={},r=a.createContext(s);function o(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);