"use strict";(self.webpackChunkpurplellama=self.webpackChunkpurplellama||[]).push([[7044],{5840:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var t=r(4848),s=r(8453);const i={},l="Getting Started",o={id:"user_guide/getting_started",title:"Getting Started",description:"Prerequisites",source:"@site/docs/user_guide/getting_started.md",sourceDirName:"user_guide",slug:"/user_guide/getting_started",permalink:"/PurpleLlama/docs/user_guide/getting_started",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/user_guide/getting_started.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/PurpleLlama/docs/intro"},next:{title:"MITRE and FRR Benchmarks",permalink:"/PurpleLlama/docs/benchmarks/mitre_benchmark"}},c={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Set up Virtual Environment",id:"set-up-virtual-environment",level:2},{value:"Running the Benchmarks",id:"running-the-benchmarks",level:2},{value:"How to use a different model supported by Anyscale or Together",id:"how-to-use-a-different-model-supported-by-anyscale-or-together",level:3},{value:"How to run benchmarks for self hosted models",id:"how-to-run-benchmarks-for-self-hosted-models",level:3},{value:"Other notes about running benchmarks",id:"other-notes-about-running-benchmarks",level:3}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Please make sure you are running python3.9 or newer versions."}),"\n",(0,t.jsxs)(n.p,{children:["Also ensure you have ",(0,t.jsx)(n.a,{href:"https://github.com/weggli-rs/weggli",children:"weggli"})," installed.\nTheir release is outdated and we're currently using revision\n",(0,t.jsx)(n.code,{children:"9d97d462854a9b682874b259f70cc5a97a70f2cc"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cargo install weggli --rev=9d97d462854a9b682874b259f70cc5a97a70f2cc --git=https://github.com/weggli-rs/weggli\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Set the environment variable ",(0,t.jsx)(n.code,{children:"WEGGLI_PATH"})," to point to the weggli execution\npath. For example, if ",(0,t.jsx)(n.code,{children:"weggli"})," is set to reference the path of the executable,\nuse the below command."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"export WEGGLI_PATH=weggli\n"})}),"\n",(0,t.jsx)(n.h2,{id:"set-up-virtual-environment",children:"Set up Virtual Environment"}),"\n",(0,t.jsx)(n.p,{children:"Set up your virtual environment and install dependencies:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 -m venv ~/.venvs/CybersecurityBenchmarks\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"source ~/.venvs/CybersecurityBenchmarks/bin/activate\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pip3 install -r CybersecurityBenchmarks/requirements.txt\n"})}),"\n",(0,t.jsx)(n.h2,{id:"running-the-benchmarks",children:"Running the Benchmarks"}),"\n",(0,t.jsxs)(n.p,{children:["The main module for benchmark is ",(0,t.jsx)(n.code,{children:"CybersecurityBenchmarks.benchmark.run"}),". Run"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python3 -m CybersecurityBenchmarks.benchmark.run --help\n"})}),"\n",(0,t.jsx)(n.p,{children:"to get a list of available options."}),"\n",(0,t.jsxs)(n.p,{children:["We simplify the following commands by setting a ",(0,t.jsx)(n.code,{children:"DATASETS"})," environment variable:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"export DATASETS=$PWD/CybersecurityBenchmarks/datasets\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Each benchmark can run tests for multiple LLMs. Our command line interface uses\nthe format ",(0,t.jsx)(n.code,{children:"<PROVIDER>::<MODEL>::<API KEY>"})," to specify an LLM to test. We\ncurrently support APIs from OPENAI, ANYSCALE, and TOGETHER. The followings are a\nfew examples:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"OPENAI::gpt-4::<API KEY>"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"OPENAI::gpt-3.5-turbo::<API KEY>"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"ANYSCALE::meta-llama/Llama-2-13b-chat-hf::<API KEY>"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"TOGETHER::togethercomputer/llama-2-7b::<API KEY>"})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-to-use-a-different-model-supported-by-anyscale-or-together",children:"How to use a different model supported by Anyscale or Together"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Lookup the models supported by\n",(0,t.jsx)(n.a,{href:"https://d3qavlo5goipcw.cloudfront.net/guides/models#select-a-model",children:"Anyscale"}),"\nor ",(0,t.jsx)(n.a,{href:"https://docs.together.ai/docs/fine-tuning-models",children:"Together"})]}),"\n",(0,t.jsxs)(n.li,{children:["Update ",(0,t.jsx)(n.code,{children:"valid_models"})," method in\n",(0,t.jsx)(n.a,{href:"https://github.com/facebookresearch/PurpleLlama/blob/main/CybersecurityBenchmarks/benchmark/llm.py",children:"llm.py"}),"\nwith the model name that you choose to test with."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-to-run-benchmarks-for-self-hosted-models",children:"How to run benchmarks for self hosted models"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Extend ",(0,t.jsx)(n.code,{children:"llm.py"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"def query(self, prompt: str) -> str:"})," Implement your inferencing logic."]}),"\n",(0,t.jsxs)(n.li,{children:["Make sure response of your llm is always a string, thats returned from the\n",(0,t.jsx)(n.code,{children:"query"})," function."]}),"\n",(0,t.jsxs)(n.li,{children:["Update supported providers in ",(0,t.jsx)(n.code,{children:"llm.create"})," method with the name of LLM you\ncreated from step (1)."]}),"\n",(0,t.jsxs)(n.li,{children:["Pass the name of your LLM in ",(0,t.jsx)(n.code,{children:"--llm-under-test"})," options in the following\nformat (",(0,t.jsx)(n.code,{children:"LLM Name::model-name::random-string"}),")"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"other-notes-about-running-benchmarks",children:"Other notes about running benchmarks"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Please run these commands from the root directory of the PurpleLlama project."}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"run-llm-in-parallel"})," makes async parallel calls to the specified LLM\nprovider. This speeds up prompt evaluation."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>o});var t=r(6540);const s={},i=t.createContext(s);function l(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);