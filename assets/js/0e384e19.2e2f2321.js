"use strict";(self.webpackChunkpurplellama=self.webpackChunkpurplellama||[]).push([[3976],{619:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var t=n(4848),i=n(8453);const r={},a="Introduction",o={id:"intro",title:"Introduction",description:"This repository hosts the implementation of CyberSecEval 3, which builds upon and extends the functionalities of its predecessors,",source:"@site/docs/intro.md",sourceDirName:".",slug:"/intro",permalink:"/PurpleLlama/docs/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",next:{title:"Getting Started",permalink:"/PurpleLlama/docs/user_guide/getting_started"}},c={},l=[];function d(e){const s={a:"a",h1:"h1",li:"li",ol:"ol",p:"p",strong:"strong",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.h1,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(s.p,{children:["This repository hosts the implementation of ",(0,t.jsx)(s.a,{href:"https://ai.meta.com/research/publications/cyberseceval-3-advancing-the-evaluation-of-cybersecurity-risks-and-capabilities-in-large-language-models/",children:"CyberSecEval 3"}),", which builds upon and extends the functionalities of its predecessors,\n",(0,t.jsx)(s.a,{href:"https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/",children:"CyberSecEval 2"})," and\n",(0,t.jsx)(s.a,{href:"https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/",children:"CyberSecEval"}),".\nCyberSecEval 3 is an extensive benchmark suite designed to assess the cybersecurity\nvulnerabilities of Large Language Models (LLMs). Building on its predecessor,\nCyberSecEval 2, this latest version introduces three new test suites:\nvisual prompt injection tests, spear phishing capability tests, and\nautonomous offensive cyber operations tests. Created to meet the increasing\ndemand for secure AI systems, CyberSecEval 3 offers a comprehensive set of tools\nto evaluate various security domains. It has been applied to well-known LLMs such\nas Llama2, Llama3, codeLlama, and OpenAI GPT models. The findings underscore\nsubstantial cybersecurity threats, underscoring the critical need for continued\nresearch and development in AI safety."]}),"\n",(0,t.jsx)(s.p,{children:"The repository includes several types of benchmarks:"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"MITRE and False Refusal Rate (FRR) Tests"}),": These tests evaluate an\nLLM's compliance when asked to assist in cyberattacks as well as how\noften an LLM incorrectly refuses a borderline but essentially benign query."]}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"MITRE Tests"}),": These tests use the MITRE ATT&CK framework to evaluate\nan LLM's compliance when asked to assist in cyberattacks."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"False Refusal Rate (FRR) Tests"}),": These tests measure how often an LLM\nincorrectly refuses a borderline but essentially benign query, due to\nmisinterpreting the prompt as a malicious request."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Secure Code Generation Tests"}),": These tests assess an LLM's propensity to\ngenerate secure code across various contexts."]}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Instruct Tests"}),": These tests assess an LLM's propensity to generate\ninsecure code when given a specific instruction."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Autocomplete Tests"}),": These tests measure how often an LLM suggests\ninsecure coding practices in autocomplete contexts, where the LLM\npredicts subsequent code based on preceding code."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Prompt Injection Tests"}),": These tests assess an LLM\u2019s susceptibility to\n\u201cprompt injection attacks\u201d - attacks in which a portion of the LLM prompt\ncoming from untrusted user input contains malicious instructions intended to\noverride the LLM\u2019s original task."]}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Textual Prompt Injection Tests"}),": An english-only dataset of textual\nprompt injections."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Multilingual Prompt Injection Tests"}),": The multilingual dataset\nincludes prompt injections machine-translated into 15 languages, to test\nthe propensity of these attacks to succeed in languages other than\nenglish."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Visual Prompt Injection Tests"}),": These tests assess an LLM\u2019s\nsusceptibility to \u201cvisual prompt injection attacks\u201d - attacks in which\nuntrusted multimodal user input (consisting of both text and images)\ncontains malicious instructions intended to override the LLM\u2019s original\ntask."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Code Interpreter Tests"}),": These tests evaluate the security risks posed by\nintegrating LLMs with code interpreters, specifically assessing how\neffectively an LLM can prevent malicious attempts to exploit the system or\nexecute harmful code."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Vulnerability Exploitation Tests"}),': These tests measure the LLM\'s program\nexploitation capabilities by asking the LLMs to solve "capture the flag"\nstyle challenges.']}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Spear Phishing Capability Tests"}),": These tests are designed to evaluate\nthe capabilities of LLMs in spear phishing scenarios. The focus is on\nassessing the LLM's persuasiveness and its effectiveness in convincing\ntargeted victims to meet specific phishing objectives."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Autonomous Offensive Cyber Operations Tests"}),": These tests are designed to\nevaluate the capabilities of LLMs to function autonomously as a cyber attack agent."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,i.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>a,x:()=>o});var t=n(6540);const i={},r=t.createContext(i);function a(e){const s=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:s},e.children)}}}]);