"use strict";(self.webpackChunkpurplellama=self.webpackChunkpurplellama||[]).push([[1873],{5972:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var t=n(4848),s=n(8453);const i={},o="Code Interpreter",a={id:"benchmarks/code_interpreter",title:"Code Interpreter",description:"Running Code Interpreter Abuse Benchmark",source:"@site/docs/benchmarks/code_interpreter.md",sourceDirName:"benchmarks",slug:"/benchmarks/code_interpreter",permalink:"/PurpleLlama/docs/benchmarks/code_interpreter",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/benchmarks/code_interpreter.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Prompt Injection",permalink:"/PurpleLlama/docs/benchmarks/prompt_injection"},next:{title:"Vulnerability Exploitation",permalink:"/PurpleLlama/docs/benchmarks/vulnerability_exploitation"}},l={},c=[{value:"Running Code Interpreter Abuse Benchmark",id:"running-code-interpreter-abuse-benchmark",level:2},{value:"Results:",id:"results",level:2},{value:"Code Interpreter Abuse Results",id:"code-interpreter-abuse-results",level:3}];function p(e){const r={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.h1,{id:"code-interpreter",children:"Code Interpreter"}),"\n",(0,t.jsx)(r.h2,{id:"running-code-interpreter-abuse-benchmark",children:"Running Code Interpreter Abuse Benchmark"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{children:'python3 -m CybersecurityBenchmarks.benchmark.run \\\n   --benchmark=interpreter \\\n   --prompt-path="$DATASETS/interpreter/interpreter.json" \\\n   --response-path="$DATASETS/interpreter/interpreter_responses.json" \\\n   --judge-response-path="$DATASETS/interpreter/interpreter_judge_responses.json" \\\n   --stat-path="$DATASETS/interpreter/interpreter_stat.json" \\\n   --judge-llm="OPENAI::gpt-3.5-turbo::<YOUR API KEY>" \\\n   --llm-under-test=<SPECIFICATION_1> --llm-under-test=<SPECIFICATION_2> ...\n   [--run-llm-in-parallel]\n'})}),"\n",(0,t.jsx)(r.p,{children:"Code Interpreter Abuse benchmark is run in two steps:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.em,{children:"Processing prompts"})," - The LLM is called (in parallel, if specified) to render\nthe LLM output for each prompt."]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.em,{children:"Judging responses"})," - The judge LLM determines whether the responses are\nextremely malicious, potentially malicious, or non-malicious. This is based on\nwhether the LLM-under-test has complied with a malicious prompt. The judge LLM\nmodel is independent of the initial LLM used for processing prompts."]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"results",children:"Results:"}),"\n",(0,t.jsxs)(r.p,{children:["Once the benchmarks have run, the evaluations of each model across each language\nwill be available under the ",(0,t.jsx)(r.code,{children:"stat_path"}),":"]}),"\n",(0,t.jsx)(r.h3,{id:"code-interpreter-abuse-results",children:"Code Interpreter Abuse Results"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{children:'{\n    "model_name": {\n        "attack_type": {\n            "is_extremely_malicious": ...,\n            "is_potentially_malicious": ...,\n            "is_non_malicious": ...,\n            "total_count": ...,\n            "malicious_percentage": ...\n        },\n    }\n}\n'})}),"\n",(0,t.jsx)(r.p,{children:"Each prompt is evaluated and the model's response is categorized as either\nextremely malicious, potentially malicious, or non-malicious. The overall\nstatistics provide a summary of the model's performance, including the total\nnumber of each category and the overall malicious percentage for each type of\nattack."})]})}function u(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>a});var t=n(6540);const s={},i=t.createContext(s);function o(e){const r=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:r},e.children)}}}]);