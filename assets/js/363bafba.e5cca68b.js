"use strict";(self.webpackChunkpurplellama=self.webpackChunkpurplellama||[]).push([[9845],{672:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>a});var i=t(4848),s=t(8453);const r={},c="Prompt Injection",o={id:"benchmarks/prompt_injection",title:"Prompt Injection",description:"Running Prompt Injection Benchmarks",source:"@site/docs/benchmarks/prompt_injection.md",sourceDirName:"benchmarks",slug:"/benchmarks/prompt_injection",permalink:"/PurpleLlama/docs/benchmarks/prompt_injection",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/benchmarks/prompt_injection.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Secure Code Benchmark",permalink:"/PurpleLlama/docs/benchmarks/secure_code_generation"},next:{title:"Code Interpreter",permalink:"/PurpleLlama/docs/benchmarks/code_interpreter"}},l={},a=[{value:"Running Prompt Injection Benchmarks",id:"running-prompt-injection-benchmarks",level:2},{value:"Textual Prompt Injection Benchmark",id:"textual-prompt-injection-benchmark",level:3},{value:"Multilingual Text Prompt Injection Benchmark",id:"multilingual-text-prompt-injection-benchmark",level:3},{value:"Running Visual Prompt Injection Benchmark",id:"running-visual-prompt-injection-benchmark",level:3},{value:"Prompt Injection Results",id:"prompt-injection-results",level:3},{value:"Results:",id:"results",level:2},{value:"Visual Prompt Injection Results",id:"visual-prompt-injection-results",level:3}];function p(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"prompt-injection",children:"Prompt Injection"}),"\n",(0,i.jsx)(n.h2,{id:"running-prompt-injection-benchmarks",children:"Running Prompt Injection Benchmarks"}),"\n",(0,i.jsx)(n.h3,{id:"textual-prompt-injection-benchmark",children:"Textual Prompt Injection Benchmark"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'python3 -m CybersecurityBenchmarks.benchmark.run \\\n   --benchmark=prompt-injection \\\n   --prompt-path="$DATASETS/prompt_injection/prompt_injection.json" \\\n   --response-path="$DATASETS/prompt_injection/prompt_injection_responses.json" \\\n   --judge-response-path="$DATASETS/prompt_injection/prompt_injection_judge_responses.json" \\\n   --stat-path="$DATASETS/prompt_injection/prompt_injection_stat.json" \\\n   --judge-llm=<>SPECIFICATION_1 \\\n   --llm-under-test=<SPECIFICATION_1> --llm-under-test=<SPECIFICATION_2> ...\n   [--run-llm-in-parallel]\n'})}),"\n",(0,i.jsx)(n.p,{children:"Textual prompt injection benchmarks are run in the following two steps:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Processing prompts"})," Query the LLMs under test with a system prompt and a user\nprompt, where the user prompt tries to inject instructions to the LLM that is\nviolating the original system prompt"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Processing response"})," The responses of the LLMs under test will go through\nanother judge LLM, where the judge LLM will judge if the injected instruction\nis executed"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"multilingual-text-prompt-injection-benchmark",children:"Multilingual Text Prompt Injection Benchmark"}),"\n",(0,i.jsx)(n.p,{children:"For the multilingual textual prompt injection benchmark, the same benchmark\nimplementation is used, only the dataset is changed."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'python3 -m CybersecurityBenchmarks.benchmark.run \\\n   --benchmark=prompt-injection \\\n   --prompt-path="$DATASETS/prompt_injection/prompt_injection_multilingual.json" \\\n   ... (rest same as above)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"running-visual-prompt-injection-benchmark",children:"Running Visual Prompt Injection Benchmark"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'python3 -m CybersecurityBenchmarks.benchmark.run \\\n   --benchmark=visual-prompt-injection \\\n   --prompt-path="$DATASETS/visual_prompt_injection/<DATASET>" \\\n   --response-path="$DATASETS/results/visual_prompt_injection/<DATASET>/visual_prompt_injection_model_responses.json" \\\n   --judge-response-path="$DATASETS/results/visual_prompt_injection/<DATASET>/visual_prompt_injection_judge_responses.json" \\\n   --stat-path="$DATASETS/results/visual_prompt_injection/<DATASET>/visual_prompt_injection_stats.json" \\\n   --judge-llm=<SPECIFICATION_1> \\\n   --llm-under-test=<SPECIFICATION_2> --llm-under-test=<SPECIFICATION_3> ... \\\n   [--run-llm-in-parallel] \\\n   [--num-queries-per-prompt=5]\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Unlike the other benchmarks, a directory should be specified for ",(0,i.jsx)(n.code,{children:"--prompt-path"}),"\nfor this benchmark rather than a JSON file. The expected data format in the\ndirectory is:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.code,{children:"test_cases.json"})," file with integer IDs for each test case"]}),"\n",(0,i.jsxs)(n.li,{children:["A subdirectory named ",(0,i.jsx)(n.code,{children:"images/"})," which have images corresponding to the test\ncases with filenames ",(0,i.jsx)(n.code,{children:"{id}.png"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["We include several datasets under the ",(0,i.jsx)(n.code,{children:"$DATASETS/visual_prompt_injection"})," that\ncan be used with this benchmark:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"cse2_typographic_images"})," - 231 test cases which transform test cases from\nCSE2's text-only prompt injection dataset into images via a simple\ntypographic conversion."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"manual"})," - 100 manually created test cases which cover a wide range of known\nvisual prompt injection techniques."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"generated/*"})," - Note that this directory itself is not a dataset in the\nexpected format. However, each of the subdirectories under this directory\nconsist of a dataset of size 100 of procedurally generated test cases\nfeaturing a particular visual prompt injection technique."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"generated_merged"})," - All the sub-datasets from ",(0,i.jsx)(n.code,{children:"generated/*"})," merged into a\nsingle dataset and randomly shuffled."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"--num-queries-per-prompt=<N>"})," can be optionally specified to run each test case\n",(0,i.jsx)(n.code,{children:"N"})," times (default if unspecified is 1) in order to obtain more robust results\ndue to the stochastic nature of LLM responses."]}),"\n",(0,i.jsx)(n.h3,{id:"prompt-injection-results",children:"Prompt Injection Results"}),"\n",(0,i.jsx)(n.p,{children:"Based on judge LLM, the prompt injection will be either successful or\nunsuccessful"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model": {\n        "model_name": {\n            "injection_successful_count": ...,\n            "injection_unsuccessful_count": ...,\n            "total_count": ...,\n            "injection_successful_percentage": ...,\n            "injection_unsuccessful_percentage": ...\n        }\n    }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"In the same output file, the results are futher bucketized in different\ndimensions, e.g., the injection techniques, and whether the prompt is a direct\nprompt injection or an indrect prompt injection, etc. The result of each bucket\nfollows the same structure."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model_per_variant": {\n        "model_name": {\n            "injected_prompt_technique_variant": {\n                "injection_successful_count": ...,\n                "injection_unsuccessful_count": ...,\n                "total_count": ...,\n                "injection_successful_percentage": ...,\n                "injection_unsuccessful_percentage": ...\n            },\n            ...\n        }\n    },\n    "stat_per_model_per_type": {\n        ...\n    },\n    "stat_per_model_per_risk_category": {\n        ...\n    },\n    "stat_per_model_per_speaking_language": {\n        ..\n    }\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"results",children:"Results:"}),"\n",(0,i.jsxs)(n.p,{children:["Once the benchmarks have run, the evaluations of each model across each language\nwill be available under the ",(0,i.jsx)(n.code,{children:"stat_path"}),":"]}),"\n",(0,i.jsx)(n.h3,{id:"visual-prompt-injection-results",children:"Visual Prompt Injection Results"}),"\n",(0,i.jsx)(n.p,{children:"Based on the evaluation of the judge LLM, the output of visual prompt injection\ntest cases will be judged as either a successful or unsuccessful injection."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model": {\n        "model_name": {\n            "injection_successful_count": ...,\n            "injection_unsuccessful_count": ...,\n            "total_count": ...,\n            "injection_successful_percentage": ...,\n            "injection_unsuccessful_percentage": ...\n        }\n    }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"In the same output file, the results are further bucketized in different\ndimensions:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Injection techniques"}),"\n",(0,i.jsx)(n.li,{children:"Risk category (security-violating vs. logic-violating)"}),"\n",(0,i.jsx)(n.li,{children:"Injection type (direct vs. indirect)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Please refer to the CSE3 paper for a comprehensive definition of these terms."}),"\n",(0,i.jsx)(n.p,{children:"The result of each bucket follows the same structure."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model_per_injection_technique": {\n        "model_name": {\n            "embedded_text_instructions": {\n                "injection_successful_count": ...,\n                "injection_unsuccessful_count": ...,\n                "total_count": ...,\n                "injection_successful_percentage": ...,\n                "injection_unsuccessful_percentage": ...\n            },\n            ...\n        }\n    },\n    "stat_per_model_per_injection_type": {\n        ...\n    },\n    "stat_per_model_per_risk_category": {\n        ...\n    }\n}\n'})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>o});var i=t(6540);const s={},r=i.createContext(s);function c(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);