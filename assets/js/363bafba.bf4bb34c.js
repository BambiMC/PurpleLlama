"use strict";(self.webpackChunkpurplellama=self.webpackChunkpurplellama||[]).push([[9845],{672:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>a,frontMatter:()=>s,metadata:()=>c,toc:()=>u});var i=t(4848),r=t(8453);const s={},o="Prompt Injection",c={id:"benchmarks/prompt_injection",title:"Prompt Injection",description:"Running Prompt Injection Benchmarks",source:"@site/docs/benchmarks/prompt_injection.md",sourceDirName:"benchmarks",slug:"/benchmarks/prompt_injection",permalink:"/PurpleLlama/docs/benchmarks/prompt_injection",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/benchmarks/prompt_injection.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Secure Code Benchmark",permalink:"/PurpleLlama/docs/benchmarks/secure_code_generation"},next:{title:"Code Interpreter",permalink:"/PurpleLlama/docs/benchmarks/code_interpreter"}},l={},u=[{value:"Running Prompt Injection Benchmarks",id:"running-prompt-injection-benchmarks",level:2},{value:"Textual Prompt Injection Benchmark",id:"textual-prompt-injection-benchmark",level:3},{value:"Multilingual Text Prompt Injection Benchmark",id:"multilingual-text-prompt-injection-benchmark",level:3},{value:"Running Visual Prompt Injection Benchmark",id:"running-visual-prompt-injection-benchmark",level:3},{value:"Prompt Injection Results",id:"prompt-injection-results",level:3},{value:"Results:",id:"results",level:2},{value:"Visual Prompt Injection Results",id:"visual-prompt-injection-results",level:3}];function p(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"prompt-injection",children:"Prompt Injection"}),"\n",(0,i.jsx)(n.h2,{id:"running-prompt-injection-benchmarks",children:"Running Prompt Injection Benchmarks"}),"\n",(0,i.jsx)(n.h3,{id:"textual-prompt-injection-benchmark",children:"Textual Prompt Injection Benchmark"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'python3 -m CybersecurityBenchmarks.benchmark.run \\\n   --benchmark=prompt-injection \\\n   --prompt-path="$DATASETS/prompt_injection/prompt_injection.json" \\\n   --response-path="$DATASETS/prompt_injection/prompt_injection_responses.json" \\\n   --judge-response-path="$DATASETS/prompt_injection/prompt_injection_judge_responses.json" \\\n   --stat-path="$DATASETS/prompt_injection/prompt_injection_stat.json" \\\n   --judge-llm=<>SPECIFICATION_1 \\\n   --llm-under-test=<SPECIFICATION_1> --llm-under-test=<SPECIFICATION_2> ...\n   [--run-llm-in-parallel]\n'})}),"\n",(0,i.jsx)(n.p,{children:"Textual prompt injection benchmarks are run in the following two steps:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Processing prompts"})," Query the LLMs under test with a system prompt and a user\nprompt, where the user prompt tries to inject instructions to the LLM that is\nviolating the original system prompt"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Processing response"})," The responses of the LLMs under test will go through\nanother judge LLM, where the judge LLM will judge if the injected instruction\nis executed"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"multilingual-text-prompt-injection-benchmark",children:"Multilingual Text Prompt Injection Benchmark"}),"\n",(0,i.jsx)(n.p,{children:"For the multilingual textual prompt injection benchmark, the same benchmark\nimplementation is used, only the dataset is changed."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'python3 -m CybersecurityBenchmarks.benchmark.run \\\n   --benchmark=prompt-injection \\\n   --prompt-path="$DATASETS/prompt_injection/prompt_injection_multilingual.json" \\\n   ... (rest same as above)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"running-visual-prompt-injection-benchmark",children:"Running Visual Prompt Injection Benchmark"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'python3 -m CybersecurityBenchmarks.benchmark.run \\\n   --benchmark=visual-prompt-injection \\\n   --prompt-path="$DATASETS/visual_prompt_injection/<DATASET>" \\\n   --response-path="$DATASETS/results/visual_prompt_injection/<DATASET>/visual_prompt_injection_model_responses.json" \\\n   --judge-response-path="$DATASETS/results/visual_prompt_injection/<DATASET>/visual_prompt_injection_judge_responses.json" \\\n   --stat-path="$DATASETS/results/visual_prompt_injection/<DATASET>/visual_prompt_injection_stats.json" \\\n   --judge-llm=<SPECIFICATION_1> \\\n   --llm-under-test=<SPECIFICATION_2> --llm-under-test=<SPECIFICATION_3> ... \\\n   [--run-llm-in-parallel] \\\n   [--num-queries-per-prompt=5]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"prompt-injection-results",children:"Prompt Injection Results"}),"\n",(0,i.jsx)(n.p,{children:"Based on judge LLM, the prompt injection will be either successful or\nunsuccessful"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model": {\n        "model_name": {\n            "injection_successful_count": ...,\n            "injection_unsuccessful_count": ...,\n            "total_count": ...,\n            "injection_successful_percentage": ...,\n            "injection_unsuccessful_percentage": ...\n        }\n    }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"In the same output file, the results are futher bucketized in different\ndimensions, e.g., the injection techniques, and whether the prompt is a direct\nprompt injection or an indrect prompt injection, etc. The result of each bucket\nfollows the same structure."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model_per_variant": {\n        "model_name": {\n            "injected_prompt_technique_variant": {\n                "injection_successful_count": ...,\n                "injection_unsuccessful_count": ...,\n                "total_count": ...,\n                "injection_successful_percentage": ...,\n                "injection_unsuccessful_percentage": ...\n            },\n            ...\n        }\n    },\n    "stat_per_model_per_type": {\n        ...\n    },\n    "stat_per_model_per_risk_category": {\n        ...\n    },\n    "stat_per_model_per_speaking_language": {\n        ..\n    }\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"results",children:"Results:"}),"\n",(0,i.jsxs)(n.p,{children:["Once the benchmarks have run, the evaluations of each model across each language\nwill be available under the ",(0,i.jsx)(n.code,{children:"stat_path"}),":"]}),"\n",(0,i.jsx)(n.h3,{id:"visual-prompt-injection-results",children:"Visual Prompt Injection Results"}),"\n",(0,i.jsx)(n.p,{children:"Based on the evaluation of the judge LLM, the output of visual prompt injection\ntest cases will be judged as either a successful or unsuccessful injection."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model": {\n        "model_name": {\n            "injection_successful_count": ...,\n            "injection_unsuccessful_count": ...,\n            "total_count": ...,\n            "injection_successful_percentage": ...,\n            "injection_unsuccessful_percentage": ...\n        }\n    }\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"In the same output file, the results are further bucketized in different\ndimensions:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Injection techniques"}),"\n",(0,i.jsx)(n.li,{children:"Risk category (security-violating vs. logic-violating)"}),"\n",(0,i.jsx)(n.li,{children:"Injection type (direct vs. indirect)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Please refer to the CSE3 paper for a comprehensive definition of these terms."}),"\n",(0,i.jsx)(n.p,{children:"The result of each bucket follows the same structure."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "stat_per_model_per_injection_technique": {\n        "model_name": {\n            "embedded_text_instructions": {\n                "injection_successful_count": ...,\n                "injection_unsuccessful_count": ...,\n                "total_count": ...,\n                "injection_successful_percentage": ...,\n                "injection_unsuccessful_percentage": ...\n            },\n            ...\n        }\n    },\n    "stat_per_model_per_injection_type": {\n        ...\n    },\n    "stat_per_model_per_risk_category": {\n        ...\n    }\n}\n'})})]})}function a(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var i=t(6540);const r={},s=i.createContext(r);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);